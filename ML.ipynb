{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxuhGqjX0EoQ"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn # Neural network modules (layers, activations, etc.)\n",
        "import torch.optim as optim  # Optimization algorithms (Adam, SGD, etc.)\n",
        "import torchvision  # Datasets, models, and transforms for computer vision\n",
        "import torchvision.transforms as transforms  # Image transformations (augmentation, normalization)\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR # Learning rate scheduler\n",
        "import matplotlib.pyplot as plt  # Plotting training curves\n",
        "\n",
        "\n",
        "# Define training data augmentation for better generalisation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
        "    transforms.RandomCrop(32, padding=4),   # Random crop with padding\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Colour variations\n",
        "    transforms.RandomRotation(15), # Slight rotation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),  # CIFAR10 stats\n",
        "])\n",
        "\n",
        "#%% Task 1: Read dataset and create data loaders\n",
        "# Define test/validation transformations (no augmentation, only normalissation)\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "# Create data loaders with 128 batch size\n",
        "batch_size = 128  # Number of samples processed in one forward/backward pass\n",
        "# Dataset loading\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "#%% Task 2: Create the model\n",
        "class ExpertBranch(nn.Module):\n",
        "    def __init__(self, in_channels, k=4, reduction=16):\n",
        "        super(ExpertBranch, self).__init__()\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)  # Global average pooling\n",
        "        self.fc1 = nn.Linear(in_channels, in_channels // reduction) # First fully connected layer, to reduce channels\n",
        "        self.bn1 = nn.BatchNorm1d(in_channels // reduction)\n",
        "        self.relu = nn.LeakyReLU(0.1) # Better than ReLU for small gradients\n",
        "        self.fc2 = nn.Linear(in_channels // reduction, k)  # Output K attention weights\n",
        "        self.softmax = nn.Softmax(dim=1) # Softmax ensures weights sum to 1 (probabilistic)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avgpool(x).view(x.size(0), -1) # Flatten spatial dimensions (batch_size, channels)\n",
        "        x = self.relu(self.bn1(self.fc1(x))) # Apply FC1 + BN + LeakyReLU\n",
        "        x = self.softmax(self.fc2(x)) # Return attention weights (batch_size, k)\n",
        "        return x\n",
        "\n",
        "class BackboneBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, k=4, dropout_prob=0.1):\n",
        "        super(BackboneBlock, self).__init__()\n",
        "        self.expert_branch = ExpertBranch(in_channels, k)\n",
        "\n",
        "        # Enhanced convolutional branches with residual connections\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels), # Normalise activations\n",
        "                nn.LeakyReLU(0.1), # Non-linearity\n",
        "                nn.Dropout2d(dropout_prob), # Spatial dropout (regularisation)\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),  # Conv2 (using 3x3 kernel)\n",
        "                nn.BatchNorm2d(out_channels)  # Final normalisation\n",
        "            ) for _ in range(k)  # Create the K branches\n",
        "        ])\n",
        "\n",
        "        # Shortcut connection for residual learning\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False), # 1x1 conv for channel adjustment\n",
        "                nn.BatchNorm2d(out_channels))\n",
        "\n",
        "        self.relu = nn.LeakyReLU(0.1) # Activation after the residual addition\n",
        "        self.pool = nn.MaxPool2d(2, 2) # Spatial downsampling\n",
        "\n",
        "    def forward(self, x):\n",
        "        a = self.expert_branch(x)  # Attention weights [batch_size, k]\n",
        "        out = sum(a[:, i].view(-1, 1, 1, 1) * self.convs[i](x) for i in range(len(self.convs)))\n",
        "        out += self.shortcut(x)  # Residual connection\n",
        "        out = self.relu(out)\n",
        "        out = self.pool(out)  # Downsample and activate\n",
        "        return out\n",
        "\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        # Enhanced stem with larger initial channels\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),  # 3x3 conv (this preserves the spatial dimensions)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout2d(0.1) # Used for regularisation\n",
        "        )\n",
        "\n",
        "        # Deeper backbone with more blocks\n",
        "        self.backbone = nn.Sequential(\n",
        "            BackboneBlock(64, 128, k=4), # Eg 64 channels to 128 channels\n",
        "            BackboneBlock(128, 256, k=4),\n",
        "            BackboneBlock(256, 512, k=4)\n",
        "        )\n",
        "\n",
        "        # Enhanced classifier with more layers and dropout\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),   # Reduces the spatial dims to 1x1 (batch_size, 512, 1, 1)\n",
        "            nn.Flatten(), # Flatten to (batch_size, 512)\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.3), # High dropout (regularissation)\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 10) # Output 10 classes (CIFAR-10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)  # Initial feature extraction\n",
        "        x = self.backbone(x) # Backbone processing\n",
        "        x = self.classifier(x) # Completes final classification\n",
        "        return x\n",
        "\n",
        "#%% Task 3: Create loss and optimizer\n",
        "# Detect available device (GPU if available, else use the CPU)model = CNNModel()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=CNNModel()\n",
        "model.to(device)\n",
        "# Loss function (using CrossEntropyLoss for classification)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Optimiser (using AdamW with weight decay for L2 regularisation)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
        "\n",
        "#%% Task 4: The Training script\n",
        "def train_model(model, trainloader, testloader, criterion, optimizer, scheduler, epochs=35):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0 # Cumulative loss\n",
        "        correct_train = 0 # Correct predictions\n",
        "        total_train = 0 # Total samples\n",
        "\n",
        "        for images, labels in trainloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad() # Clear gradients\n",
        "            outputs = model(images)  # Forward pass\n",
        "            loss = criterion(outputs, labels) # Computes the losss\n",
        "            loss.backward() # Backpropagation\n",
        "            optimizer.step()  # Updates the weights\n",
        "\n",
        "            running_loss += loss.item() # Accumulates the loss\n",
        "            _, predicted = torch.max(outputs, 1) # Get the predicted class\n",
        "            total_train += labels.size(0) # Updates the total samples\n",
        "            correct_train += (predicted == labels).sum().item()  # Update the number of correct predictions\n",
        "\n",
        "        scheduler.step() # Update the learning rate\n",
        "        avg_train_loss = running_loss / len(trainloader)  # Calculates average training loss\n",
        "        train_accuracy = 100 * correct_train / total_train  # Training accuracy (%)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Test evaluation\n",
        "        model.eval()  # Evaluates model\n",
        "        test_loss = 0.0\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        with torch.no_grad(): # Disables the gradient computation\n",
        "            for images, labels in testloader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_test += labels.size(0)\n",
        "                correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_test_loss = test_loss / len(testloader) # Averaeg validation loss\n",
        "        test_accuracy = 100 * correct_test / total_test  # Validation accuracy (%)\n",
        "        test_losses.append(avg_test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}, \"\n",
        "              f\"Train Acc: {train_accuracy:.2f}%, Test Acc: {test_accuracy:.2f}%\")\n",
        "\n",
        "    return train_losses, test_losses, train_accuracies, test_accuracies\n",
        "\n",
        "# Train the model\n",
        "train_losses, test_losses, train_accuracies, test_accuracies = train_model(\n",
        "    model, trainloader, testloader, criterion, optimizer, scheduler, epochs=35)\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss', marker='o')\n",
        "plt.plot(range(1, len(test_losses)+1), test_losses, label='Test Loss', marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Test Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(range(1, len(train_accuracies)+1), train_accuracies, label='Train Accuracy', marker='o')\n",
        "plt.plot(range(1, len(test_accuracies)+1), test_accuracies, label='Test Accuracy', marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Training and Test Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final evaluation\n",
        "def evaluate_model(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Final Accuracy on CIFAR-10 test set: {accuracy:.2f}%')\n",
        "    return accuracy\n",
        "\n",
        "final_accuracy = evaluate_model(model, testloader)"
      ]
    }
  ]
}